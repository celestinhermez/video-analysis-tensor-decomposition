{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read all of our files, and get the number of frames in each one. When reading them as tensors we will truncate to the one with the smallest number of frames. I strived to take videos of the same length (~11s), but small discrepancies are bound to exist. For our particular application, truncation ought not to matter too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 328)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create VideoCapture objects\n",
    "parking_lot = cv2.VideoCapture('parking_lot.MOV')\n",
    "patio = cv2.VideoCapture('patio.MOV')\n",
    "\n",
    "# Get number of frames in each video\n",
    "parking_lot_frames = int(parking_lot.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "patio_frames = int(patio.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "parking_lot_frames, patio_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 1920\n",
      "1080 1920\n"
     ]
    }
   ],
   "source": [
    "# Get dimensions of each frame\n",
    "parking_lot_height = int(parking_lot.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "parking_lot_width = int(parking_lot.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "patio_height = int(patio.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "patio_width = int(patio.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "print(parking_lot_height, parking_lot_width)\n",
    "print(patio_height, patio_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the number of frames and the dimensions of the frames, we will need a 4D tensor (321x1080x1920x3) to hold these videos:\n",
    "- 321 for the frames of the images (we truncate the extra frames for the patio video)\n",
    "- 1080x1920 for the height and width of the images\n",
    "- 3 for the RGB color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read all frames of a video in an array\n",
    "def read_frames(video_capture, max_frames):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    video_capture: an OpenCV VideoCapture object whose frames we want to read\n",
    "    max_frames: the maximum number of frames we want to read\n",
    "    \n",
    "    OUTPUT:\n",
    "    array of all the frames until max_frames\n",
    "    \"\"\"\n",
    "    # Initialize empty array\n",
    "    frames_array = []\n",
    "    \n",
    "    # Keep track of the frame number\n",
    "    frame_nb = 0\n",
    "    \n",
    "    # iterate through the frames and append them to the array\n",
    "    while video_capture.isOpened() and frame_nb < max_frames:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames_array.append(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        frame_nb += 1\n",
    "    \n",
    "    # release the video capture\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # return the array\n",
    "    return(frames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the videos\n",
    "parking_lot_array = read_frames(video_capture=parking_lot, max_frames=parking_lot_frames)\n",
    "patio_array = read_frames(video_capture=patio, max_frames=parking_lot_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create tensors out of the NumPy arrays for use with the TensorLy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors from matrices\n",
    "parking_lot_tensor = tl.tensor(parking_lot_array)\n",
    "patio_tensor = tl.tensor(patio_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up later steps, we randomly select frames of the tensors to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "random_frames = random.sample(range(0, 320), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these random frames to subset the tensors\n",
    "subset_parking_lot = parking_lot_tensor[random_frames,:,:,:]\n",
    "subset_patio = patio_tensor[random_frames,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_parking_lot = subset_parking_lot.astype('d')\n",
    "subset_patio = subset_patio.astype('d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural way of comparing two tensors is to compute the norm of the difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831354.197367074"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parking_patio_naive_diff = tl.norm(subset_parking_lot - subset_patio)\n",
    "parking_patio_naive_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tensors, we can perform Tucker decomposition to get a more robust representation of the tensor (using the resulting core tensor) to get rid of noise and get a better sense of the similarity between two videos.\n",
    "\n",
    "The main tuning parameter is the n-rank of the tensor. If we were seeking the optimal decomposition, AIC criterion could be used to choose the best hyper parameter. Nevertheless, in this specific context we are not looking for an optimal setting, rather something that is usable. Besides, we need similar dimensions across tensors to be able to make comparisons.\n",
    "\n",
    "For this reason, we chose n-rank of [2,2,2,2] for all tensors and compare the resulting core tensors. This also helps by reducing the computational complexity of our operations (trying out n-rank of [5,5,5,5] will exceed the capabilities of LAPACK, which is used under the hood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get core tensor for the parking lot video\n",
    "core_parking_lot, factors_parking_lot = tucker(subset_parking_lot, ranks = [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get core tensor for the patio video\n",
    "core_patio, factors_patio= tucker(subset_patio, ranks = [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3719206.800381976"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make comparisons\n",
    "parking_patio_diff = tl.norm(core_parking_lot - core_patio)\n",
    "parking_patio_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
