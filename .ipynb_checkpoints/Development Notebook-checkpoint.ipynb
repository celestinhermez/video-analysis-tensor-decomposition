{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all of the files, and get the number of frames in each one. When reading them as tensors we will truncate to the smallest number of frames. I strived to take videos of the same length (~11s), but small discrepancies are bound to exist. For our particular application, truncation ought not to matter too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 328, 314)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create VideoCapture objects\n",
    "parking_lot = cv2.VideoCapture('parking_lot.MOV')\n",
    "patio = cv2.VideoCapture('patio.MOV')\n",
    "commute = cv2.VideoCapture('commute.MOV')\n",
    "\n",
    "# Get number of frames in each video\n",
    "parking_lot_frames = int(parking_lot.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "patio_frames = int(patio.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "commute_frames = int(commute.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "parking_lot_frames, patio_frames, commute_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 1920\n",
      "1080 1920\n",
      "1080 1920\n"
     ]
    }
   ],
   "source": [
    "# Get dimensions of each frame\n",
    "parking_lot_height = int(parking_lot.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "parking_lot_width = int(parking_lot.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "patio_height = int(patio.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "patio_width = int(patio.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "commute_height = int(commute.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "commute_width = int(commute.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "print(parking_lot_height, parking_lot_width)\n",
    "print(patio_height, patio_width)\n",
    "print(commute_height, commute_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the number of frames and the dimensions of the frames, we need a 4D tensor (314x1080x1920x3) to hold these videos:\n",
    "- 314 for the frames of the images (we truncate the extra frames for the patio and parking lot videos)\n",
    "- 1080x1920 for the height and width of the images\n",
    "- 3 for the RGB color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read all frames of a video in an array\n",
    "def read_frames(video_capture, max_frames):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "    video_capture: an OpenCV VideoCapture object whose frames we want to read\n",
    "    max_frames: the maximum number of frames we want to read\n",
    "    \n",
    "    OUTPUT:\n",
    "    array of all the frames until max_frames\n",
    "    \"\"\"\n",
    "    # Initialize empty array\n",
    "    frames_array = []\n",
    "    \n",
    "    # Keep track of the frame number\n",
    "    frame_nb = 0\n",
    "    \n",
    "    # iterate through the frames and append them to the array\n",
    "    while video_capture.isOpened() and frame_nb < max_frames:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames_array.append(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        frame_nb += 1\n",
    "    \n",
    "    # release the video capture\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # return the array\n",
    "    return(frames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the videos\n",
    "parking_lot_array = read_frames(video_capture=parking_lot, max_frames=commute_frames)\n",
    "patio_array = read_frames(video_capture=patio, max_frames=commute_frames)\n",
    "commute_array = read_frames(video_capture=commute, max_frames=commute_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create tensors out of the NumPy arrays with the TensorLy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors from matrices\n",
    "parking_lot_tensor = tl.tensor(parking_lot_array)\n",
    "patio_tensor = tl.tensor(patio_array)\n",
    "commute_tensor = tl.tensor(commute_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up later steps, we randomly select 50 frames of the tensors to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "random_frames = random.sample(range(0, commute_frames), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these random frames to subset the tensors\n",
    "subset_parking_lot = parking_lot_tensor[random_frames,:,:,:]\n",
    "subset_patio = patio_tensor[random_frames,:,:,:]\n",
    "subset_commute = commute_tensor[random_frames, :, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to double, otherwise Tucker decomposition will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert three tensors to double\n",
    "subset_parking_lot = subset_parking_lot.astype('d')\n",
    "subset_patio = subset_patio.astype('d')\n",
    "subset_commute = subset_commute.astype('d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural way of comparing two tensors is to compute the norm of the difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between parking and patio tensors is 1832804, 1862115 between parking and commute and 1840975 between patio and commute\n"
     ]
    }
   ],
   "source": [
    "# Parking and patio\n",
    "parking_patio_naive_diff = tl.norm(subset_parking_lot - subset_patio)\n",
    "\n",
    "# Parking and commute\n",
    "parking_commute_naive_diff = tl.norm(subset_parking_lot - subset_commute)\n",
    "\n",
    "# Patio and commute\n",
    "patio_commute_naive_diff = tl.norm(subset_patio - subset_commute)\n",
    "\n",
    "# Print our differences\n",
    "print(\"The difference between parking and patio tensors is {}, {} between parking and commute and {} between patio and commute\".format(int(parking_patio_naive_diff), int(parking_commute_naive_diff), \n",
    "                                         int(patio_commute_naive_diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tensors, we can perform Tucker decomposition to get a more robust representation (using the resulting core tensor). This rids us of noise and we get a better sense of the similarity between two videos.\n",
    "\n",
    "The main tuning parameter is the n-rank of the tensor. If we were seeking the optimal decomposition, AIC criterion could be used to choose the best value of the hyperparameter. Nevertheless, in this specific context we are not looking for an optimal setting, rather something that is usable. Besides, we need similar dimensions across tensors to be able to make comparisons.\n",
    "\n",
    "For this reason, we chose n-rank of [2,2,2,2] for all tensors and compare the resulting core tensors. Choosing this somewhat small n-rank helps by limiting the computational complexity of our operations (trying out n-rank of [5,5,5,5] will exceed the capabilities of LAPACK, which is used under the hood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get core tensor for the parking lot video\n",
    "core_parking_lot, factors_parking_lot = tucker(subset_parking_lot, ranks = [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get core tensor for the patio video\n",
    "core_patio, factors_patio = tucker(subset_patio, ranks = [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get core tensor for the commute video\n",
    "core_commute, factors_commute = tucker(subset_commute, ranks = [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3707771"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare core parking lot and patio\n",
    "parking_patio_diff = tl.norm(core_parking_lot - core_patio)\n",
    "int(parking_patio_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675670"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare core parking lot and commute\n",
    "parking_commute_diff= tl.norm(core_parking_lot - core_commute)\n",
    "int(parking_commute_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3630173"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare core patio and commute\n",
    "patio_commute_diff = tl.norm(core_patio - core_commute)\n",
    "int(patio_commute_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Leveraging Tucker decomposition allows us to make robust comparisons between videos by extracting the core tensor, the main information contained in it.\n",
    "\n",
    "This has very broad applications (recommender systems, material science) but also needs a lot of computing power, with some potential for parallelization for this to be used at scale.\n",
    "\n",
    "For more information on the mathematical underpinning of tensor decomposition as well as broader context on this analysis, please refer to the Medium article linked in the associated README file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
